{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def preprocess(data_dirs, data_file,name):\n",
    "    # all_ped_data would be a dictionary with mapping from each ped to their\n",
    "    # trajectories given by matrix 3 x numPoints with each column\n",
    "    # in the order x, y, frameId\n",
    "    # Pedestrians from all datasets are combined\n",
    "    # Dataset pedestrian indices are stored in dataset_indices\n",
    "    all_ped_data={}\n",
    "    dataset_indices=[]\n",
    "    current_ped = 0\n",
    "    # For each dataset\n",
    "    for directory in data_dirs:\n",
    "        # Define the path to its respective csv file\n",
    "        #PETS2009-S2L1-mmundo.csv\n",
    "        #'PETS09-S2L1.txt'\n",
    "        file_path = os.path.join(directory,name )\n",
    "\n",
    "        # Cargar datos desde el archivo csv\n",
    "        # Los datos son una matriz 4x numTrajPoints\n",
    "        \n",
    "        \n",
    "        data = np.genfromtxt(file_path, delimiter=',')\n",
    "       \n",
    "        \n",
    "        # Obtenga el numero de peatones en el conjunto de datos actual\n",
    "        uni=np.unique(data[:,1])    \n",
    "        numPeds=np.size(np.unique(data[:,1]))\n",
    "     \n",
    "        print(\"peatones.............\")\n",
    "        print(numPeds)\n",
    "       \n",
    "        # Para cada peaton en el conjunto de datos\n",
    "        for ped in range(1, numPeds+1):\n",
    "            #Son los datos de la persona ped\n",
    "            traj = data[ data[:, 1] == ped]\n",
    "            #Esta como (x,y,frame_Id)\n",
    "            traj = traj[:, [2,3,0]]\n",
    "            \n",
    "            #Esta como [[x,...],[y,...],[Frame_Id,..]]\n",
    "            traj=[list(traj[:,0]),list(traj[:,1]),list(traj[:,2])]\n",
    "            all_ped_data[current_ped + ped] = np.array(traj)\n",
    "\n",
    "        # Current dataset done\n",
    "        dataset_indices.append(current_ped+numPeds)\n",
    "        current_ped += numPeds\n",
    "\n",
    "    # Los datos completos son una tupla de todos los datos de peatones y los indices de datos del conjunto de datos.\n",
    "    complete_data = (all_ped_data, dataset_indices)\n",
    "    # Almacena los datos completos en el archivo pickle\n",
    "    f = open(data_file, \"wb\")\n",
    "    pickle.dump(complete_data, f, protocol=2)\n",
    "    f.close()\n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed(data_file,seq_length_obs,batch_size):\n",
    "    \n",
    "    # cargar los datos\n",
    "    f = open(data_file, \"rb\")\n",
    "    raw_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Obtengo los peatones\n",
    "    all_ped_data =raw_data[0]\n",
    " \n",
    "    #all_ped_data es el vector {[[],[],[]],...,[[],[],[]]} tantos como pedestrian hayan\n",
    "    # No usamos dataset_indices por ahora\n",
    "    # dataset_indices = self.raw_data[1]\n",
    "\n",
    "    # Construimos data con secuencias(o trayectorias) del largo than seq_length_obs\n",
    "    data = []\n",
    "    counter = 0\n",
    "\n",
    "    # para cada peaton en data\n",
    "    for ped in all_ped_data:\n",
    "        \n",
    "        #ped es un numero entero y va de 1 hasta la cantidad total de pedestrian\n",
    "        # Extract his trajectory\n",
    "        traj = all_ped_data[ped]\n",
    "        \n",
    "           \n",
    "        # If the length of the trajectory is greater than seq_length (+2 as we need both source and target data)\n",
    "        #solo se toman las trajectorias de longitud mayor a seq_length+2\n",
    "        if traj.shape[1] >= (seq_length_obs+1):\n",
    "            # TODO: (Improve) Store only the (x,y) coordinates for now\n",
    "               \n",
    "            data.append(traj[[0, 1], :].T)\n",
    "            #print(traj[[0, 1], :].T)\n",
    "            # Number of batches this datapoint is worth\n",
    "            #print(traj.shape[1] )\n",
    "            #print(int(traj.shape[1] / ((seq_length_obs+1))))\n",
    "            counter += int(traj.shape[1] / ((seq_length_obs+1)))\n",
    "        \n",
    "    # Calculate the number of batches (each of batch_size) in the data\n",
    "    #counter tiene la cantidad de bloques de 8 pasos\n",
    "        \n",
    "    num_batches = int(counter /batch_size)\n",
    "    #cada bache tiene batch_size conjuntos donde cada conjunto tiene datos de length+2\n",
    "    return data,num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peatones.............\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "data_dirs = ['../data1/eth/univ']\n",
    "datasets = [0]\n",
    "\n",
    "used_data_dirs = [data_dirs[x] for x in datasets]\n",
    "#Directorio de datos donde reside el archivo pickle preprocesado\n",
    "data_dir = '../data1'\n",
    "\n",
    "#Defina la ruta del archivo en el que deben almacenarse los datos.\n",
    "data_file = os.path.join(data_dir, \"datos_limpios_pets.cpkl\")\n",
    "\n",
    "name ='pixel_pos.csv'\n",
    "# If the file doesn't exist already or if forcePreProcess is true\n",
    "#se usa preprocesscsv para los csv y preprocess para el .txt\n",
    "\n",
    "data = preprocess(used_data_dirs, data_file,name)\n",
    "datos,numero = load_preprocessed(data_file,12,1)#los ultimos dos valores noimportan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_promedio(predicted_traj, true_traj, observed_length):\n",
    "    \n",
    "    error = np.zeros(len(true_traj) - observed_length)\n",
    "    # PARA CADA PUNTO EN LA TRAYECTORIA PREDICHA\n",
    "    for i in range(observed_length, len(true_traj)):\n",
    "        # The predicted position\n",
    "        pred_pos = predicted_traj[i]\n",
    "        #The true position\n",
    "        true_pos = true_traj[i]\n",
    "\n",
    "        # The euclidean distance is the error\n",
    "        error[i-observed_length] = np.linalg.norm(true_pos - pred_pos)\n",
    "\n",
    "    # Return the mean error\n",
    "    return np.mean(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_desplazamiento_final(predicted_traj, true_traj):\n",
    "    tam = len(predicted_traj)\n",
    "    return np.linalg.norm(predicted_traj[tam-1]-true_traj[tam-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta funcion construye secuencias con solo la parte observada y secuencias completa\n",
    "# es decir secuencia de longitud obs+pred\n",
    "\n",
    "def secuencia_pred(seq_length_obs,data,seq_length_pred):\n",
    "    \n",
    "    tamano = int(len(data))\n",
    "    X,Y_true = [],[]\n",
    "    # se recorre todo los datos de test\n",
    "    for j in range(tamano):\n",
    "        traj = data[j]\n",
    "        \n",
    "        lon = traj.shape[0]-seq_length_obs-seq_length_pred\n",
    "        #lon=data.shape[0]-seq_length_obs-seq_length_pred\n",
    "        for i in range(0,lon+1):\n",
    "            a = traj[i:(i +seq_length_obs ), :]\n",
    "            \n",
    "            X.append(a)\n",
    "            # esta parte tiene tanto la parte observada como \n",
    "            b = traj[i: (i+seq_length_obs+seq_length_pred), :]\n",
    "        \n",
    "            Y_true.append(b)\n",
    "    return np.array(X),np.array(Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolacion(datos_obs):\n",
    "    x = datos_obs[:,0]\n",
    "    y = datos_obs[:,1]\n",
    "    \n",
    "    mx = (x[len(x)-1]-x[0])/(len(x)-1)\n",
    "    my = (y[len(x)-1]-y[0])/(len(y)-1)\n",
    "    \n",
    "    x_next = x[0]+mx*(len(x)+1)\n",
    "    y_next = y[0]+my*(len(y)+1)\n",
    "    \n",
    "    po = np.array([[x_next,y_next]])\n",
    "    return po\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_en_pixeles(datos, seq_length_obs, seq_length_pred):\n",
    "    \n",
    "    X,Y_true = secuencia_pred(seq_length_obs,datos,seq_length_pred)\n",
    "    total_error = 0.0\n",
    "    total_final = 0.0\n",
    "    todo = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        traj_obs = X[i]\n",
    "        traj_pred = X[i]\n",
    "    \n",
    "        #print(Y_true[i])\n",
    "        \n",
    "        for j in range(seq_length_pred):\n",
    "        \n",
    "            predict = extrapolacion(traj_obs)\n",
    "            #print(predict.shape)\n",
    "            traj_obs = np.concatenate((traj_obs[1:len(traj_obs)],predict),axis = 0)\n",
    "            traj_pred = np.concatenate((traj_pred,predict),axis = 0)\n",
    "        \n",
    "        traj_pre =  np.column_stack((720*traj_pred[:,0],576*traj_pred[:,1]))\n",
    "        traj_tr = np.column_stack((720*Y_true[i][:,0],576*Y_true[i][:,1]))\n",
    "          \n",
    "        diff = traj_pre[seq_length_obs:]-traj_tr[seq_length_obs:]\n",
    "        diff = diff**2\n",
    "        diff = np.sqrt(np.sum(diff,axis=1))\n",
    "        todo.append(diff)\n",
    "        \n",
    "        total_error += error_promedio(traj_pre,traj_tr, seq_length_obs)\n",
    "        total_final += error_desplazamiento_final(traj_pre, traj_tr)\n",
    "             \n",
    "    error_modelo = total_error/len(X)\n",
    "    error_fde_modelo = total_final/len(X)\n",
    "    \n",
    "    ade=[t for o in todo for t in o]\n",
    "    \n",
    "    print('---------Error--------')\n",
    "    print('ADE')\n",
    "    print(np.mean(ade))\n",
    "    \n",
    "    print('Error promedio')\n",
    "    print(error_modelo)\n",
    "    \n",
    "    print('FDE')\n",
    "    print(error_fde_modelo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Error--------\n",
      "ADE\n",
      "72.92540046577285\n",
      "Error promedio\n",
      "72.92540046577284\n",
      "FDE\n",
      "133.03326473439895\n"
     ]
    }
   ],
   "source": [
    "sample_en_pixeles(datos,8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
