{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em2aVoafL-ML"
   },
   "source": [
    "# Para ejecutar en Google Colab en Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1626216648545,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "qJhAwN4z7jJ9",
    "outputId": "4bddcff6-8536-41d9-d105-2dbc23fecaf6"
   },
   "outputs": [],
   "source": [
    "# Montamos el Drive al Notebook\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1626216650801,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "pnCMtpvc7n1s",
    "outputId": "79a7a3a2-6c71-4c17-c89f-85e98bc28297"
   },
   "outputs": [],
   "source": [
    "# Verificamos el directorio en el que nos encontramos\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1626216652277,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "OoAkWfZL7oCL",
    "outputId": "a6b806df-70b5-41ed-b70f-7393858de298"
   },
   "outputs": [],
   "source": [
    "# Cambiamos de directorio al Drive\n",
    "import os\n",
    "os.chdir(\"drive/My Drive/PruebasCOLAB3/OF-PathPred/tests\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYovpqgdMNjU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GM2YmtlMTXp"
   },
   "source": [
    "# Inicio de CÃ³digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5361,
     "status": "ok",
     "timestamp": 1626216661406,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "2hOKb3HV6YKm"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys,os\n",
    "''' TF_CPP_MIN_LOG_LEVEL\n",
    "0 = all messages are logged (default behavior)\n",
    "1 = INFO messages are not printed\n",
    "2 = INFO and WARNING messages are not printeds\n",
    "3 = INFO, WARNING, and ERROR messages are not printed\n",
    "'''\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "sys.path.append('../path_prediction/')\n",
    "sys.path.append('../')\n",
    "import math,numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Important imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets_utils import setup_loo_experiment, get_testing_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3979,
     "status": "ok",
     "timestamp": 1626216667631,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "4uB-rhO4sCDM",
    "outputId": "342baf1b-b6fd-43b7-db83-62e16398b617"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1626216670573,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "yVOfPyeJGsOj"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# The only datasets that can use add_social are those of ETH/UCY\n",
    "# The only datasets that can use add_kp are PETS2009-S2L1, TOWN-CENTRE\n",
    "class Experiment_Parameters:\n",
    "    def __init__(self,add_social=False,add_kp=False,obstacles=False):\n",
    "        # Maximum number of persons in a frame\n",
    "        self.person_max =70\n",
    "        # Observation length (trajlet size)\n",
    "        self.obs_len    = 8\n",
    "        # Prediction length\n",
    "        self.pred_len   = 12\n",
    "        # Flag to consider social interactions\n",
    "        self.add_social = add_social\n",
    "        # Number of key points\n",
    "        self.kp_num     = 18\n",
    "        # Key point flag\n",
    "        self.add_kp     = add_kp\n",
    "        # Obstacles flag\n",
    "        self.obstacles    = obstacles\n",
    "        self.intersection = False\n",
    "        self.delim        = ','\n",
    "        self.output_representation = 'dxdy' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1626216672102,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "9KnuOybYI8pr"
   },
   "outputs": [],
   "source": [
    "class Model_Parameters(object):\n",
    "    \"\"\"Model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, add_attention=True, add_kp=False, add_social=False, output_representation='dxdy'):\n",
    "        # -----------------\n",
    "        # Observation/prediction lengths\n",
    "        self.obs_len        = 8\n",
    "        self.pred_len       = 12\n",
    "        self.seq_len        = self.obs_len + self.pred_len\n",
    "        self.add_kp         = add_kp\n",
    "        self.add_social     = add_social\n",
    "        self.add_attention  = add_attention\n",
    "        self.stack_rnn_size = 2\n",
    "        self.output_representation = output_representation\n",
    "        self.output_var_dirs= 0\n",
    "        # Key points\n",
    "        self.kp_size        = 18\n",
    "        # Optical flow\n",
    "        self.flow_size      = 64\n",
    "        # For training\n",
    "        self.num_epochs     = 35\n",
    "        self.batch_size     = 256  # batch size 512\n",
    "        self.use_validation = True\n",
    "        # Network architecture\n",
    "        self.P              =   2 # Dimensions of the position vectors\n",
    "        self.enc_hidden_size= 256                  # Default value in NextP\n",
    "        self.dec_hidden_size= self.enc_hidden_size # Default value in NextP\n",
    "        self.emb_size       = 128  # Default value in NextP\n",
    "        self.dropout_rate   = 0.3 # Default value in NextP\n",
    "\n",
    "        #self.activation_func= tf.nn.tanh\n",
    "        self.multi_decoder  = False\n",
    "        self.modelname      = 'gphuctl'\n",
    "        self.optimizer      = 'adam'\n",
    "        self.initial_lr     = 0.01\n",
    "        # MC dropout\n",
    "        self.is_mc_dropout         = False\n",
    "        self.mc_samples            = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1626216675247,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "oYyijVou6YKq"
   },
   "outputs": [],
   "source": [
    "# Load the default parameters\n",
    "experiment_parameters = Experiment_Parameters(add_social=False,add_kp=False,obstacles=False)\n",
    "\n",
    "dataset_dir   = \"../datasets/\"\n",
    "dataset_names = ['eth-hotel','eth-univ','ucy-zara01','ucy-zara02','ucy-univ']\n",
    "#dataset_names = ['eth-hotel','eth-univ','ucy-zara01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54443,
     "status": "ok",
     "timestamp": 1626216963560,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "bySxfWOc6YKs",
    "outputId": "5529c0c1-2034-4030-9dbd-d6bf5fa79019"
   },
   "outputs": [],
   "source": [
    "# Load the dataset and perform the split\n",
    "training_data, validation_data, test_data, test_homography = setup_loo_experiment('ETH_UCY',dataset_dir,dataset_names,2,experiment_parameters,use_pickled_data=False)\n",
    "\n",
    "print('obs_traj: ',training_data['obs_traj'].shape)\n",
    "print('obs_traj_rel: ',training_data['obs_traj_rel'].shape)\n",
    "print('obs_traj_theta: ',training_data['obs_traj_theta'].shape)\n",
    "print('pred_traj: ',training_data['pred_traj'].shape)\n",
    "print('pred_traj_rel: ',training_data['pred_traj_rel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1626217018285,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "00GpCrbg6YKt"
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Model parameters\n",
    "model_parameters = Model_Parameters(add_attention=True,add_kp=experiment_parameters.add_kp,add_social=experiment_parameters.add_social,output_representation=experiment_parameters.output_representation)\n",
    "\n",
    "model_parameters.num_epochs     = 40\n",
    "model_parameters.output_var_dirs= 0\n",
    "model_parameters.is_mc_dropout  = False\n",
    "model_parameters.initial_lr     = 0.03\n",
    "model_parameters.dropout_rate   = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1626217024480,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "ZF0jBL9pWppF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Creamos la clase para el dataset\n",
    "class traj_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, Xrel_Train, Yrel_Train, X_Train, Y_Train, transform=None):\n",
    "        self.Xrel_Train = Xrel_Train\n",
    "        self.Yrel_Train = Yrel_Train\n",
    "        self.X_Train = X_Train\n",
    "        self.Y_Train = Y_Train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_Train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        xrel = self.Xrel_Train[idx]\n",
    "        yrel = self.Yrel_Train[idx]\n",
    "        x = self.X_Train[idx]\n",
    "        y = self.Y_Train[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            y = self.transform(y)\n",
    "            xrel = self.transform(xrel)\n",
    "            yrel = self.transform(yrel)\n",
    "\n",
    "        return xrel, yrel, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1626217038070,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "jnCdyKzuW_gI"
   },
   "outputs": [],
   "source": [
    "# Creamos el dataset para torch\n",
    "train_data = traj_dataset(training_data['obs_traj_rel'], training_data['pred_traj_rel'],training_data['obs_traj'], training_data['pred_traj'])\n",
    "val_data = traj_dataset(validation_data['obs_traj_rel'], validation_data['pred_traj_rel'],validation_data['obs_traj'], validation_data['pred_traj'])\n",
    "test_data = traj_dataset(test_data['obs_traj_rel'], test_data['pred_traj_rel'], test_data['obs_traj'], test_data['pred_traj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1626217045689,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "5j23o7zkGh1A"
   },
   "outputs": [],
   "source": [
    "# Form batches\n",
    "batched_train_data = torch.utils.data.DataLoader( train_data, batch_size = model_parameters.batch_size, shuffle=True)\n",
    "batched_val_data =  torch.utils.data.DataLoader( val_data, batch_size = model_parameters.batch_size, shuffle=True)\n",
    "batched_test_data =  torch.utils.data.DataLoader( test_data, batch_size = model_parameters.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../bayesian-torch/')\n",
    "from bayesian_torch.layers import LinearReparameterization\n",
    "from bayesian_torch.layers import LSTMReparameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1626217050435,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "IRwsEyE2emM4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "prior_mu = 0.0\n",
    "prior_sigma = 1.0\n",
    "posterior_mu_init = 0.0\n",
    "posterior_rho_init = -4.0\n",
    "posterior_rho_init = -3.0 # 0.006715348489117967 # 0.01814992791780978 # 0.04858735157374196\n",
    "\n",
    "len_trainset = 15481\n",
    "len_valset = 1720\n",
    "\n",
    "\n",
    "class LSTM_variational(nn.Module):\n",
    "    def __init__(self, in_size,  embedding_dim, hidden_dim, output_size):\n",
    "        super(LSTM_variational, self).__init__()\n",
    "\n",
    "        # Linear layer\n",
    "        self.embedding = LinearReparameterization(\n",
    "            in_features = in_size,\n",
    "            out_features = embedding_dim, # 128\n",
    "            prior_mean = prior_mu,\n",
    "            prior_variance = prior_sigma,\n",
    "            posterior_mu_init = posterior_mu_init,\n",
    "            posterior_rho_init = posterior_rho_init,\n",
    "        )\n",
    "\n",
    "        # LSTM layer encoder\n",
    "        self.lstm1 = LSTMReparameterization(\n",
    "            in_features = embedding_dim,\n",
    "            out_features = hidden_dim, # 256\n",
    "            prior_mean = prior_mu,\n",
    "            prior_variance = prior_sigma,\n",
    "            posterior_mu_init = posterior_mu_init,\n",
    "            posterior_rho_init = posterior_rho_init,\n",
    "        )\n",
    "\n",
    "        # LSTM layer decoder\n",
    "        self.lstm2 = LSTMReparameterization(\n",
    "            in_features = embedding_dim,\n",
    "            out_features = hidden_dim, # 256\n",
    "            prior_mean = prior_mu,\n",
    "            prior_variance = prior_sigma,\n",
    "            posterior_mu_init = posterior_mu_init,\n",
    "            posterior_rho_init = posterior_rho_init,\n",
    "        )\n",
    "        # Linear layer\n",
    "        self.decoder = LinearReparameterization(\n",
    "            in_features = hidden_dim,\n",
    "            out_features = output_size, # 2\n",
    "            prior_mean = prior_mu,\n",
    "            prior_variance = prior_sigma,\n",
    "            posterior_mu_init = posterior_mu_init,\n",
    "            posterior_rho_init = posterior_rho_init,\n",
    "        )\n",
    "        self.loss_fun = nn.MSELoss()\n",
    "    # \n",
    "    def forward(self, X, y, training=False, num_mc=1):\n",
    "      \n",
    "        output_ = []\n",
    "        kl_     = []\n",
    "        # \n",
    "        nbatches = len(X)\n",
    "        # Last position in the trajectory\n",
    "        x_last = X[:,-1,:].view(nbatches, 1, -1) \n",
    "\n",
    "        # Monte Carlo iterations\n",
    "        for mc_run in range(num_mc):\n",
    "            kl_sum = 0\n",
    "            # Layers\n",
    "            emb, kl = self.embedding(X) # encoder for batch\n",
    "            kl_sum += kl\n",
    "            lstm_out, (hn1, cn1), kl = self.lstm1(emb)\n",
    "            kl_sum += kl\n",
    "\n",
    "            # Iterate for each time step\n",
    "            pred = [] \n",
    "            for i, target in enumerate(y.permute(1,0,2)):\n",
    "                emb_last, kl = self.embedding(x_last) # encoder for last position\n",
    "                kl_sum += kl\n",
    "                lstm_out, (hn2, cn2), kl = self.lstm2(emb_last, (hn1[:,-1,:],cn1[:,-1,:]))\n",
    "                kl_sum += kl\n",
    "\n",
    "                # Decoder and Prediction\n",
    "                dec, kl = self.decoder(hn2)\n",
    "                kl_sum += kl\n",
    "                t_pred = dec + x_last\n",
    "                pred.append(t_pred)\n",
    "\n",
    "                # Update the last position\n",
    "                if training:\n",
    "                    x_last = target.view(len(target), 1, -1)\n",
    "                    len_evaldataset = len_trainset\n",
    "                else:\n",
    "                    x_last = t_pred\n",
    "                    len_evaldataset = len_valset\n",
    "                hn1 = hn2\n",
    "                cn1 = cn2\n",
    "\n",
    "            # Concatenate the trajectories preds\n",
    "            pred = torch.cat(pred, dim=1)\n",
    "\n",
    "            # save to list\n",
    "            output_.append(pred)\n",
    "            kl_.append(kl_sum)\n",
    "\n",
    "        pred    = torch.mean(torch.stack(output_), dim=0)\n",
    "        kl_loss = torch.mean(torch.stack(kl_), dim=0)\n",
    "\n",
    "        # Calculate of nl loss\n",
    "        nll_loss = self.loss_fun(pred, y)\n",
    "        # Concatenate the predictions and return\n",
    "        return pred, nll_loss, kl_loss\n",
    "\n",
    "    def predict(self, X, dim_pred= 1):\n",
    "\n",
    "      # Copy data\n",
    "      x = X\n",
    "      # Last position traj\n",
    "      x_last = X[:,-1,:].view(len(x), 1, -1) \n",
    "\n",
    "      kl_sum = 0\n",
    "      # Layers\n",
    "      emb, kl = self.embedding(X) # encoder for batch\n",
    "      kl_sum += kl\n",
    "      lstm_out, (hn1, cn1), kl = self.lstm1(emb)\n",
    "      kl_sum += kl\n",
    "\n",
    "      # Iterate for each time step\n",
    "      pred = [] \n",
    "      for i in range(dim_pred):\n",
    "          emb_last, kl = self.embedding(x_last) # encoder for last position\n",
    "          kl_sum += kl\n",
    "          lstm_out, (hn2, cn2), kl = self.lstm2(emb_last, (hn1[:,-1,:],cn1[:,-1,:]))\n",
    "          kl_sum += kl\n",
    "\n",
    "          # Decoder and Prediction\n",
    "          dec, kl = self.decoder(hn2)\n",
    "          kl_sum += kl\n",
    "          t_pred = dec + x_last\n",
    "          pred.append(t_pred)\n",
    "\n",
    "          # Update the last position\n",
    "          x_last = t_pred\n",
    "          hn1 = hn2\n",
    "          cn1 = cn2\n",
    "\n",
    "      # Concatenate the predictions and return\n",
    "      return torch.cat(pred, dim=1).detach().cpu().numpy(), kl_sum\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_determinista(nn.Module):\n",
    "    def __init__(self, in_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(LSTM_determinista, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.embedding = nn.Linear(in_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        #self.loss_fun = nn.CrossEntropyLoss()\n",
    "        self.loss_fun = nn.MSELoss()\n",
    "\n",
    "    def forward(self, X, y, training=False):\n",
    "\n",
    "        # Copy data\n",
    "        x = X\n",
    "        # Last position traj\n",
    "        x_last = X[:,-1,:].view(len(x), 1, -1) \n",
    "\n",
    "        # Layers\n",
    "        emb = self.embedding(X) # encoder for batch\n",
    "        lstm_out, (hn1, cn1) = self.lstm1(emb.permute(1,0,2)) # LSTM for batch [seq_len, batch, input_size] \n",
    "\n",
    "        loss = 0\n",
    "        pred = []\n",
    "        for i, target in enumerate(y.permute(1,0,2)):\n",
    "            emb_last = self.embedding(x_last) # encoder for last position\n",
    "            lstm_out, (hn2, cn2) = self.lstm2(emb_last.permute(1,0,2), (hn1,cn1)) # lstm for last position with hidden states from batch\n",
    "\n",
    "            # Decoder and Prediction\n",
    "            dec = self.decoder(hn2.permute(1,0,2))\n",
    "            t_pred = dec + x_last\n",
    "            pred.append(t_pred)\n",
    "\n",
    "            # Calculate of loss\n",
    "            loss += self.loss_fun(t_pred, target.view(len(target), 1, -1))\n",
    "\n",
    "            # Update the last position\n",
    "            if training:\n",
    "                x_last = target.view(len(target), 1, -1)\n",
    "            else:\n",
    "                x_last = t_pred\n",
    "            hn1 = hn2\n",
    "            cn1 = cn2\n",
    "\n",
    "        # Concatenate the predictions and return\n",
    "        return torch.cat(pred, dim=1), loss\n",
    "\n",
    "    def predict(self, X, dim_pred= 1):\n",
    "\n",
    "        # Copy data\n",
    "        x = X\n",
    "        # Last position traj\n",
    "        x_last = X[:,-1,:].view(len(x), 1, -1) \n",
    "\n",
    "        # Layers\n",
    "        emb = self.embedding(X) # encoder for batch\n",
    "        lstm_out, (hn1, cn1) = self.lstm1(emb.permute(1,0,2)) # LSTM for batch [seq_len, batch, input_size] \n",
    "\n",
    "        loss = 0\n",
    "        pred = []\n",
    "        for i in range(dim_pred):\n",
    "            emb_last = self.embedding(x_last) # encoder for last position\n",
    "            lstm_out, (hn2, cn2) = self.lstm2(emb_last.permute(1,0,2), (hn1,cn1)) # lstm for last position with hidden states from batch\n",
    "\n",
    "            # Decoder and Prediction\n",
    "            dec = self.decoder(hn2.permute(1,0,2))\n",
    "            t_pred = dec + x_last\n",
    "            pred.append(t_pred)\n",
    "\n",
    "            # Update the last position\n",
    "            x_last = t_pred\n",
    "            hn1 = hn2\n",
    "            cn1 = cn2\n",
    "\n",
    "        # Concatenate the predictions and return\n",
    "        return torch.cat(pred, dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rho(sigma, delta):\n",
    "    \"\"\"\n",
    "    sigma is represented by softplus function  'sigma = log(1 + exp(rho))' to make sure it \n",
    "    remains always positive and non-transformed 'rho' gets updated during backprop.\n",
    "    \"\"\"\n",
    "    rho = torch.log(torch.expm1(delta * torch.abs(sigma)) + 1e-20)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOPED_layer(layer, det_layer, delta):\n",
    "    \"\"\"\n",
    "    Set the priors and initialize surrogate posteriors of Bayesian NN with Empirical Bayes\n",
    "    MOPED (Model Priors with Empirical Bayes using Deterministic DNN)\n",
    "    Reference:\n",
    "    [1] Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo.\n",
    "        Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes. AAAI 2020.\n",
    "    \"\"\"\n",
    "    if (str(layer) == 'Conv2dReparameterization()'):\n",
    "        #set the priors\n",
    "        print(str(layer))\n",
    "        layer.prior_weight_mu = det_layer.weight.data\n",
    "        if layer.prior_bias_mu is not None:\n",
    "            layer.prior_bias_mu = det_layer.bias.data\n",
    "\n",
    "        #initialize surrogate posteriors\n",
    "        layer.mu_kernel.data = det_layer.weight.data\n",
    "        layer.rho_kernel.data = get_rho(det_layer.weight.data, delta)\n",
    "        if layer.mu_bias is not None:\n",
    "            layer.mu_bias.data = det_layer.bias.data\n",
    "            layer.rho_bias.data = get_rho(det_layer.bias.data, delta)\n",
    "\n",
    "    elif (isinstance(layer, nn.Conv2d)):\n",
    "        print(str(layer))\n",
    "        layer.weight.data = det_layer.weight.data\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data = det_layer.bias.data2\n",
    "\n",
    "    elif (str(layer) == 'LinearReparameterization()'):\n",
    "        print(str(layer))\n",
    "        layer.prior_weight_mu = det_layer.weight.data\n",
    "        if layer.prior_bias_mu is not None:\n",
    "            layer.prior_bias_mu = det_layer.bias.data\n",
    "\n",
    "        #initialize the surrogate posteriors\n",
    "\n",
    "        layer.mu_weight.data = det_layer.weight.data\n",
    "        layer.rho_weight.data = get_rho(det_layer.weight.data, delta)\n",
    "        if layer.mu_bias is not None:\n",
    "            layer.mu_bias.data = det_layer.bias.data\n",
    "            layer.rho_bias.data = get_rho(det_layer.bias.data, delta)\n",
    "\n",
    "    elif str(layer).startswith('Batch'):\n",
    "        #initialize parameters\n",
    "        print(str(layer))\n",
    "        layer.weight.data = det_layer.weight.data\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data = det_layer.bias.data\n",
    "        layer.running_mean.data = det_layer.running_mean.data\n",
    "        layer.running_var.data = det_layer.running_var.data\n",
    "        layer.num_batches_tracked.data = det_layer.num_batches_tracked.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1626217128372,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "-2DsuYRjGrvw"
   },
   "outputs": [],
   "source": [
    "# Model Variational\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTM_variational(2,128,256,2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Deterministic\n",
    "#det_model = torch.nn.DataParallel(det_resnet.__dict__[args.arch](pretrained=True))\n",
    "det_model = LSTM_determinista(2,128,256,2)\n",
    "\n",
    "# Cargamos el Modelo\n",
    "det_model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic5.pth\"))\n",
    "det_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.2\n",
    "\n",
    "print(\"MOPED enabled\")\n",
    "for (idx_1, layer_1), (det_idx_1, det_layer_1) in zip( enumerate(model.children()), enumerate(det_model.children())):\n",
    "    MOPED_layer(layer_1, det_layer_1, delta)\n",
    "    for (idx_2, layer_2), (det_idx_2, det_layer_2) in zip( enumerate(layer_1.children()), enumerate(det_layer_1.children())):\n",
    "        MOPED_layer(layer_2, det_layer_2, delta)\n",
    "\n",
    "        for (idx_3, layer_3), (det_idx_3, det_layer_3) in zip( enumerate(layer_2.children()), enumerate(det_layer_2.children())):\n",
    "            MOPED_layer(layer_3, det_layer_3, delta)\n",
    "            for (idx_4, layer_4), (det_idx_4, det_layer_4) in zip( enumerate(layer_3.children()), enumerate(det_layer_3.children())):\n",
    "                MOPED_layer(layer_4, det_layer_4, delta)\n",
    "                for (idx_5, layer_5), (det_idx_5, det_layer_5) in zip( enumerate(layer_4.children()), enumerate(det_layer_4.children())):\n",
    "                    MOPED_layer(layer_5, det_layer_5, delta)\n",
    "                    for (idx_6, layer_6), (det_idx_6, det_layer_6) in zip( enumerate(layer_5.children()), enumerate(det_layer_5.children())):\n",
    "                        MOPED_layer(layer_6, det_layer_6, delta)\n",
    "model.state_dict()\n",
    "del det_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903961,
     "status": "ok",
     "timestamp": 1626218033804,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "2g_bvInYjAJZ",
    "outputId": "d83ede94-42e2-4494-fd21-3f30485d61ef"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Training the Model\n",
    "optimizer = optim.SGD(model.parameters(), lr=model_parameters.initial_lr)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.015)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.08) #nan\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.07) #nan \n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.06) #nan \n",
    "\n",
    "epochs = model_parameters.num_epochs\n",
    "num_mc = 10\n",
    "\n",
    "nl_loss_ = []\n",
    "kl_loss_ = []\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    print(\"----- \")\n",
    "    print(\"epoch: \", epoch)\n",
    "    error = 0\n",
    "    total = 0\n",
    "    M     = len(batched_train_data)\n",
    "    for batch_idx, (data, target, _a , _b) in enumerate(batched_train_data):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            data  = data.to(device)\n",
    "            target=target.to(device)\n",
    "\n",
    "        # Step 2. Run our forward pass and compute the losses\n",
    "        pred, nl_loss, kl_loss = model(data, target, training=True, num_mc=num_mc)\n",
    "        \n",
    "        nl_loss_.append(nl_loss.detach().item())\n",
    "        kl_loss_.append(kl_loss.detach().item())\n",
    "        \n",
    "        # TODO: Divide by the batch size\n",
    "        #pi     = (2.0**(M-batch_idx))/(2.0**M-1) #  Blundell?\n",
    "        loss   = nl_loss+ kl_loss\n",
    "        error += loss.detach().item()\n",
    "        total += len(target)\n",
    "\n",
    "        # Step 3. Compute the gradients, and update the parameters by\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Average training loss: {:.3e}\".format(error/total))\n",
    "\n",
    "    # Validation\n",
    "    error = 0\n",
    "    total = 0\n",
    "    M     = len(batched_val_data)\n",
    "    for batch_idx, (data_val, target_val, _ , _) in enumerate(batched_val_data):\n",
    "        if torch.cuda.is_available():\n",
    "            data_val  = data_val.to(device)\n",
    "            target_val=target_val.to(device)\n",
    "        pred_val, nl_loss, kl_loss = model(data_val, target_val)\n",
    "        pi     = (2.0**(M-batch_idx))/(2.0**M-1) # From Blundell\n",
    "        loss   = nl_loss+ pi*kl_loss\n",
    "        error += loss.detach().item()\n",
    "        total += len(target_val)\n",
    "\n",
    "    print(\"Average validation loss: {:.3e}\".format(error/total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99A6ayLglGmV"
   },
   "outputs": [],
   "source": [
    "plt.plot(nl_loss_,\"--b\", label=\"nl_loss\")\n",
    "plt.plot(kl_loss_,\"--r\", label=\"kl_loss\")\n",
    "plt.title(\"Loss training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nl_loss_,\"--b\", label=\"nl_loss\")\n",
    "plt.title(\"Loss training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kl_loss_,\"--r\", label=\"kl_loss\")\n",
    "plt.title(\"Loss training\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el Modelo\n",
    "torch.save(model.state_dict(), \"../training_checkpoints/model_MOPED.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1626218044464,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "LtBmwWGBHV4b"
   },
   "outputs": [],
   "source": [
    "def plot_traj(pred_traj, obs_traj_gt, pred_traj_gt, test_homography, background):\n",
    "    print(\"-----\")\n",
    "    homography = np.linalg.inv(test_homography)\n",
    "\n",
    "    # Convert it to absolute (starting from the last observed position)\n",
    "    displacement = np.cumsum(pred_traj, axis=0)\n",
    "    this_pred_out_abs = displacement + np.array([obs_traj_gt[-1].numpy()])\n",
    "\n",
    "    obs   = image_to_world_xy(obs_traj_gt, homography, flip=False)\n",
    "    gt    = image_to_world_xy(pred_traj_gt, homography, flip=False)\n",
    "    gt = np.concatenate([obs[-1,:].reshape((1,2)), gt],axis=0)\n",
    "    tpred   = image_to_world_xy(this_pred_out_abs, homography, flip=False)\n",
    "    tpred = np.concatenate([obs[-1,:].reshape((1,2)), tpred],axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(background)\n",
    "    plt.plot(obs[:,0],obs[:,1],\"-b\", linewidth=2, label=\"Observations\")\n",
    "    plt.plot(gt[:,0], gt[:,1],\"-r\", linewidth=2, label=\"Ground truth\")\n",
    "    plt.plot(tpred[:,0],tpred[:,1],\"-g\", linewidth=2, label=\"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.title('Trajectory samples')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1HCeVlXQLYZVwBDinWWjWtxv6QEqFgd2u"
    },
    "executionInfo": {
     "elapsed": 61967,
     "status": "ok",
     "timestamp": 1626218110872,
     "user": {
      "displayName": "Mario Xavier Canche Uc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiOcz_C04Q59vq9Nh21kQKQXSGnCbhsfXYEXe1R4w=s64",
      "userId": "15987765480396593536"
     },
     "user_tz": 300
    },
    "id": "30joaw2kMWDY",
    "outputId": "4c9ae6b1-9a48-4b16-a59b-612ab8fd3024"
   },
   "outputs": [],
   "source": [
    "from obstacles import image_to_world_xy\n",
    "\n",
    "num_samples = 30\n",
    "num_monte_carlo = 20\n",
    "i = 1 # sample of batch\n",
    "bck = np.load('background.npy')\n",
    "\n",
    "# Testing\n",
    "cont = 0\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_test_data):\n",
    "    print(\"-----\")\n",
    "    print(cont)\n",
    "    if torch.cuda.is_available():\n",
    "        datarel_test  = datarel_test.to(device)\n",
    "        targetrel_test= targetrel_test.to(device)\n",
    "        data_test     = data_test.to(device)\n",
    "        target_test   = target_test.to(device)        \n",
    "    homography = np.linalg.inv(test_homography)\n",
    "    \n",
    "    obs_traj_gt  = data_test[i,:,:]\n",
    "    pred_traj_gt = target_test[i,:,:]\n",
    "    obs   = image_to_world_xy(obs_traj_gt.cpu(), homography, flip=False)\n",
    "    gt    = image_to_world_xy(pred_traj_gt.cpu(), homography, flip=False)\n",
    "    gt = np.concatenate([obs[-1,:].reshape((1,2)), gt], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(bck)\n",
    "    plt.plot(obs[:,0],obs[:,1],\"-b\", linewidth=2, label=\"Observations\")\n",
    "    plt.plot(gt[:,0], gt[:,1],\"-r\", linewidth=2, label=\"Ground truth\")\n",
    "\n",
    "    # prediction\n",
    "    for mc_run in range(num_monte_carlo):\n",
    "        pred, kl = model.predict(datarel_test, dim_pred=12)\n",
    "        # ploting \n",
    "        #plot_traj(pred[i,:,:], data_test[i,:,:], target_test[i,:,:], test_homography, bck)\n",
    "\n",
    "        pred_traj = pred[i,:,:]\n",
    "\n",
    "        # Convert it to absolute (starting from the last observed position)\n",
    "        displacement      = np.cumsum(pred_traj, axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([obs_traj_gt[-1].cpu().numpy()])\n",
    "\n",
    "        tpred = image_to_world_xy(this_pred_out_abs, homography, flip=False)\n",
    "        tpred = np.concatenate([obs[-1,:].reshape((1,2)), tpred], axis=0)\n",
    "\n",
    "        if mc_run == 0:\n",
    "            plt.plot(tpred[:,0],tpred[:,1],\"-g\", linewidth=2, label=\"Prediction\")\n",
    "        else:\n",
    "            plt.plot(tpred[:,0],tpred[:,1],\"-g\", linewidth=2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Trajectory samples')\n",
    "    plt.savefig(\"traj_variational_1_4\"+str(cont)+\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    cont += 1\n",
    "        \n",
    "    if cont == num_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpU2HvaQKObj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRpI157sKOeG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n38rE4UKKOhC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prueba_con_torch_variacional.ipynb",
   "provenance": [
    {
     "file_id": "19NEdCOFkz192E98RYrt3dC_jtWhqp8BR",
     "timestamp": 1612389852697
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
